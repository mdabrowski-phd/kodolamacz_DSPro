{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b86d70",
   "metadata": {},
   "source": [
    "# Analiza tekstu - Tokenizacja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f77bf",
   "metadata": {},
   "source": [
    "**Tokenizacja** - proces przekształacania ciągu znaków lub dokumentu na mniejsze fragmenty nazywane tokenami. \n",
    "\n",
    "Tokenami często są pojedyncze słowa, ale nie jest to regułą. W zależności od kontekstu realizowanego zadania za token możemy chcieć przyjąć zdania albo pojedyncze litery. Możemy też stworzyć własne reguły tokenizacji.\n",
    "\n",
    "**Przykłady zastosowania:**\n",
    "- rozbicie słów lub zdań na mniejsze części,\n",
    "- odseparowanie znaków interpunkcyjnych.\n",
    "- odseparowanie hashtagów w tweecie\n",
    "- usuwanie niechcianych tokenów\n",
    "\n",
    "Tokenizacja stanowi najczęściej jeden z etapów wstępnego przetwarzanie tekstu przygotowującego tekst do dalszego przetwarzania/ustrukturyzowania. Jedną z częściej wykorzystywanych w pythonie bibliotek do tokenizacji jest [NLTK](https://www.nltk.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14838ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:14:00.457759Z",
     "start_time": "2023-07-16T09:13:57.451339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mdabrowski-\n",
      "[nltk_data]     phd/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6cd55",
   "metadata": {},
   "source": [
    "**Biblioteka NLTK posiada kilka tokenizerów:**\n",
    "- `word_tokenize` - wyodrębnia z tekstu słowa (ciągi znaków rozdzielone białymi znakami lub znakami interpunkcyjnym, przy czym znaki interpunkcji traktowane są jako odrębne słowa).\n",
    "- `sent_tokenize` - wyodrębnia z tekstu zdania\n",
    "- `regexp_tokenize` - wyodrębnia z tekstu tokeny zdefiniowane za pomocą wprowadzonego wyrażenia regularnego\n",
    "- `TweetTokenizer` - klasa do wyodrębniania z tekstu tweetów\n",
    "\n",
    "*(you can think of `TweetTokenizer` as a subset of `word_tokenize`. `TweetTokenizer` keeps hashtags intact while `word_tokenize` doesn't)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c0e7f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:16:31.804400Z",
     "start_time": "2023-07-16T09:16:31.798206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BlanklineTokenizer', 'LegalitySyllableTokenizer', 'LineTokenizer', 'MWETokenizer', 'NLTKWordTokenizer', 'PunktSentenceTokenizer', 'RegexpTokenizer', 'ReppTokenizer', 'SExprTokenizer', 'SpaceTokenizer', 'StanfordSegmenter', 'SyllableTokenizer', 'TabTokenizer', 'TextTilingTokenizer', 'ToktokTokenizer', 'TreebankWordDetokenizer', 'TreebankWordTokenizer', 'TweetTokenizer', 'WhitespaceTokenizer', 'WordPunctTokenizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_treebank_word_tokenizer', 'api', 'blankline_tokenize', 'casual', 'casual_tokenize', 'destructive', 'legality_principle', 'line_tokenize', 'load', 'mwe', 'punkt', 're', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'repp', 'sent_tokenize', 'sexpr', 'sexpr_tokenize', 'simple', 'sonority_sequencing', 'stanford_segmenter', 'string_span_tokenize', 'texttiling', 'toktok', 'treebank', 'util', 'word_tokenize', 'wordpunct_tokenize']\n"
     ]
    }
   ],
   "source": [
    "print(dir(nltk.tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67b476",
   "metadata": {},
   "source": [
    "## Wyodrębnianie słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866582c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:44:22.532035Z",
     "start_time": "2023-07-16T09:44:22.526958Z"
    }
   },
   "outputs": [],
   "source": [
    "# importujemy word_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9480876b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:44:24.708340Z",
     "start_time": "2023-07-16T09:44:24.692350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użyce\n",
    "res = word_tokenize(\"Hello, world!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47be80f",
   "metadata": {},
   "source": [
    "## Wyodrębnianie zdań"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313da85b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:44:36.557765Z",
     "start_time": "2023-07-16T09:44:36.553043Z"
    }
   },
   "outputs": [],
   "source": [
    "# importujemy sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdcaebd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:44:42.662663Z",
     "start_time": "2023-07-16T09:44:42.653644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't use my uncle's tools anymore.\n",
      "Another sentence in english.\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użycie\n",
    "sentences = \"I don't use my uncle's tools anymore. Another sentence in english.\"\n",
    "\n",
    "res = sent_tokenize(sentences)\n",
    "\n",
    "for sentence in res:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a96f90",
   "metadata": {},
   "source": [
    "## Wyodrębnienie tokena na podstawie wyrażenia regularnego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940ab616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:45:12.512005Z",
     "start_time": "2023-07-16T09:45:12.507439Z"
    }
   },
   "outputs": [],
   "source": [
    "# importujemy regexp_tokenize\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0fe8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:56:49.133167Z",
     "start_time": "2023-07-16T09:56:49.110848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'don', 't', 'use', 'my', 'uncle', 's', 'tools', 'anymore', 'Another', 'sentence', 'in', 'english']\n",
      "['I', 'do', \"n't\", 'use', 'my', 'uncle', \"'s\", 'tools', 'anymore', '.', 'Another', 'sentence', 'in', 'english', '.']\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użycie\n",
    "res1 = regexp_tokenize(sentences, '\\w+')  # a-z A-Z 0-9 _\n",
    "print(res1)\n",
    "\n",
    "res2 = word_tokenize(sentences)  # inteligentna tokenizacja, uwzględnia fragmenty niosące informacje!\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a493e",
   "metadata": {},
   "source": [
    "## Wyodrębnienie tweeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82bedbec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T09:46:19.220658Z",
     "start_time": "2023-07-16T09:46:19.216296Z"
    }
   },
   "outputs": [],
   "source": [
    "# importujemy (klasę) TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a96ea6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T10:23:14.043601Z",
     "start_time": "2023-07-16T10:23:14.035758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ready', '?', '#vcut'], ['Ohmmmmmmyyyyyyyyggghghhhhhhhgggggggggdhdhsjsixudbslsogbdsisgshdbxidjdbdidhdifjfiri', '#GRAMMYs', '@BTS']]\n"
     ]
    }
   ],
   "source": [
    "# podstawowe użycie\n",
    "tweets = [\n",
    "    'ready? #vcut',\n",
    "    'Ohmmmmmmyyyyyyyyggghghhhhhhhgggggggggdhdhsjsixudbslsogbdsisgshdbxidjdbdidhdifjfiri #GRAMMYs @BTS',\n",
    "]\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

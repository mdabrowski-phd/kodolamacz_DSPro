{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d13041",
   "metadata": {},
   "source": [
    "# Analiza tekstu - ciąg dalszy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404636a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.293Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa522e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca1957",
   "metadata": {},
   "source": [
    "### Tokenizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5be489",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.297Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = \"The cat is in the box. The cat likes the box. The box is over the cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68ed78",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.299Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizacja\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentences)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097fc65e",
   "metadata": {},
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939f257",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.303Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [token.lower() for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaface81",
   "metadata": {},
   "source": [
    "### Usunięcie tokenów, które nie są alfanumeryczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966f5c3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.305Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3070341",
   "metadata": {},
   "source": [
    "### Usunięcie stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e11ddb",
   "metadata": {},
   "source": [
    "**Stopwords** - najczęściej występujące słowa w języku, które nie niosą ze sobą żadnej konkretnej treści."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7253097",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.308Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c694b15",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.310Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede4836",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.312Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [token for token in tokens if token not in stopwords_list]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397e2a",
   "metadata": {},
   "source": [
    "### Lematyzacja (alternatywnie: stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef30479",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.314Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea28c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.316Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inicjalizacja WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183ae5e",
   "metadata": {},
   "source": [
    "# Techniki osadzania włów (_word embeddings_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8eaac",
   "metadata": {},
   "source": [
    "## Model \"Bag of Words\" (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021bee1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.318Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(tokens)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79140040",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.320Z"
    }
   },
   "outputs": [],
   "source": [
    "# dwa najczęściej występujące tokeny w tekście\n",
    "print(c.most_common(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347e67c",
   "metadata": {},
   "source": [
    "### Biblioteka Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310dc5a",
   "metadata": {},
   "source": [
    "Biblioteka ``Gensim`` jest inną popularną biblioteką do przetwarzania tekstu w Pythonie, która pozwala nam w prosty sposób budować korpusy i słowniki. **Korpus** to zbiór tekstów, na któym wykonujemy zadania przetwarzania języka naturalnego. Składa się z pojedynczych **dokumentów**. Zbiór słów występujących w korpusie nazywamy jego **słownikiem**. Do tokenizacji oraz oczyszczania tekstu użyjemy biblioteki `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e95d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.323Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nasz dokument (lista napisów)\n",
    "my_documents = [\n",
    "    'Nearly all great ideas follow a similar creative process and this article explains how this process works. Understanding this is important because creative thinking is one of the most useful skills you can possess.',\n",
    "    'Not doing something will always be faster than doing it. This statement reminds me of the old computer programming saying, Remember that there is no code faster than no code.',\n",
    "    'He went on to become a trailblazer in the field of photography and held over 70 patents by the end of his career. His story of creativity and innovation, which I will share now, is a useful case study for understanding the 5 key steps of the creative process.',\n",
    "    'He spent the rest of the decade experimenting with new photography techniques and learning about cameras, printers, and optics.',   \n",
    "]\n",
    "\n",
    "# preprocessing (lowercase, usuwam znaki interpunkcyjne i stopwords)\n",
    "tokenized_docs = [word_tokenize(article.lower()) for article in my_documents]\n",
    "\n",
    "for idx, article_word_list in enumerate(tokenized_docs):\n",
    "    \n",
    "    new_article_word_list = []\n",
    "    for token in article_word_list:\n",
    "        \n",
    "        if token.isalpha() and token not in stopwords_list:\n",
    "            new_article_word_list.append(token)\n",
    "    \n",
    "    tokenized_docs[idx] = new_article_word_list\n",
    "\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7afa10e",
   "metadata": {},
   "source": [
    "Z tak przygotowanego zestawy tokenów możemy stworzyć **słownik** (model _bag of words_), czyli jednoznacznie przyporządkować liczbę każdemu tokenowi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb28c9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Słownik (mapa)\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169c3ff",
   "metadata": {},
   "source": [
    "Słownik posiada metodę `doc2bow()`, któa zwraca rozkład _bag of words_ przekazanego do niej dokumentu. Zliczane są wyłącznie tokeny występujące w słowniku. Wynikowa lista składa się z dwuelementowych tupli, w któych pierwszy element to id tokena w słowniku, a drugi element to liczba wystąpień tego tokena w przekazanym do metody dokumencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232404d6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.327Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = dictionary.doc2bow(tokenized_docs[0])\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6682a",
   "metadata": {},
   "source": [
    "Następnie z tak przygotowanego słownika możemy otrzymać rozkład  _bag of words_ dla całego **korpusu**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334a114",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.328Z"
    }
   },
   "outputs": [],
   "source": [
    "# stworzony korpus należy przetworzyć na postać 'DataFrame', aby użyć modeli ML z pakietu 'sklearn'\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(article) for article in tokenized_docs]\n",
    "print(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abde8a",
   "metadata": {},
   "source": [
    "**Powyższy korpus** jest trochę czymś innym niż to co przeważnie mamy na myśli mówiąć **korpus języka - czyli zestaw dokumentów**. `Gensim` używa prostego modelu **BoW** za pomocą którego przekształca każdy dokument w **BoW** użwając **id tokenów** i częstość występowania tokenu w dokumencie. Za pomocą `Gensim` w kilku linijkach możemy otrzymać nowy **korpus** i **BoW**.\n",
    "\n",
    "I ten korpus można łatwo zapisywać, aktualizować i ponownie wykorzystywać dzięki narzędziom biblioteki `Gensim`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e5dd02",
   "metadata": {},
   "source": [
    "## Model TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ddbb8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.330Z"
    }
   },
   "outputs": [],
   "source": [
    "print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532f49a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.332Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf_corpus = TfidfModel(bow_corpus)\n",
    "print(tfidf_corpus[bow_corpus[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec616137",
   "metadata": {},
   "source": [
    "Mimo, że _token_ o **id=15** występuje w dokumencie **1 raz** (czyli dokłądnie tyle samo, co tokeny o **id=1, 2, 3, ...**) to wartość przyporządkowana jemu w **tf-idf** jest znacznie niższa. Widocznie ten token występuje częściej w innych dokumentach od pozostałych tokenów pierwszego dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789b176",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.334Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a621dc",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0329f96",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.337Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "ldamodel = LdaModel(bow_corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310b450",
   "metadata": {},
   "source": [
    "## Moduł `scikit-learn`: modele BoW i TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4592c1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-21T09:25:15.340Z"
    }
   },
   "outputs": [],
   "source": [
    "# wersja z pakietu 'scikit-learn'\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# nasz dokument (lista napisów)\n",
    "my_documents = [\n",
    "    'Nearly all great ideas follow a similar creative process and this article explains how this process works. Understanding this is important because creative thinking is one of the most useful skills you can possess.',\n",
    "    'Not doing something will always be faster than doing it. This statement reminds me of the old computer programming saying, Remember that there is no code faster than no code.',\n",
    "    'He went on to become a trailblazer in the field of photography and held over 70 patents by the end of his career. His story of creativity and innovation, which I will share now, is a useful case study for understanding the 5 key steps of the creative process.',\n",
    "    'He spent the rest of the decade experimenting with new photography techniques and learning about cameras, printers, and optics.',   \n",
    "]\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(my_documents)\n",
    "v = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=v)  # ... i po uprzednim oczyszczeniu danych można trenować model ML"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model trained using 300-dimensional pretrained FastText English word vectors released by [Facebook](https://www.kaggle.com/yekenot/fasttext-crawl-300d-2m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-08-29T14:19:39.115782Z",
     "iopub.status.busy": "2023-08-29T14:19:39.115455Z",
     "iopub.status.idle": "2023-08-29T14:19:39.122380Z",
     "shell.execute_reply": "2023-08-29T14:19:39.121280Z",
     "shell.execute_reply.started": "2023-08-29T14:19:39.115720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout,Embedding,Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping # warto zobaczyć w oryginalnym notebooku użycie dodatkowych callbacków\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:05:30.544485Z",
     "iopub.status.busy": "2023-08-29T14:05:30.544148Z",
     "iopub.status.idle": "2023-08-29T14:05:30.548810Z",
     "shell.execute_reply": "2023-08-29T14:05:30.548048Z",
     "shell.execute_reply.started": "2023-08-29T14:05:30.544428Z"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "n_embeddings = 300\n",
    "n_features = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W kodzie istotnym obiektem był `tokenizer.word_index`, czyli mapowanie ze słów do id słowa zapisanego w sekwencjach zbioru danych. My też musimy uzyskać taki obiekt. W dokumentacji można było znaleźć funkcję https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index, którą najłatwiej użyć wykorzystując gotowy fragment kodu skopiowany i uzupełniony poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:05:34.502705Z",
     "iopub.status.busy": "2023-08-29T14:05:34.502383Z",
     "iopub.status.idle": "2023-08-29T14:05:41.085518Z",
     "shell.execute_reply": "2023-08-29T14:05:41.084535Z",
     "shell.execute_reply.started": "2023-08-29T14:05:34.502644Z"
    }
   },
   "outputs": [],
   "source": [
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(\n",
    "    start_char=start_char, oov_char=oov_char, index_from=index_from,\n",
    "    num_words=5000, maxlen=maxlen, \n",
    "    path=\"/kaggle/input/keras-imdb/imdb.npz\"\n",
    ")\n",
    "\n",
    "word_index = imdb.get_word_index(path=\"/kaggle/input/imdb-word-index/imdb_word_index.json\")\n",
    "\n",
    "# filtrowanie tylko pierwszych 5000 słów\n",
    "word_index = {k: v for k,v in word_index.items() if v<n_features}\n",
    "\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:06:17.111304Z",
     "iopub.status.busy": "2023-08-29T14:06:17.110964Z",
     "iopub.status.idle": "2023-08-29T14:06:17.118669Z",
     "shell.execute_reply": "2023-08-29T14:06:17.117764Z",
     "shell.execute_reply.started": "2023-08-29T14:06:17.111248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 5001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdźmy, jakie są rozmiary word_index i inverted_word_index:\n",
    "len(word_index), len(inverted_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:13:50.645730Z",
     "iopub.status.busy": "2023-08-29T14:13:50.645409Z",
     "iopub.status.idle": "2023-08-29T14:13:52.613108Z",
     "shell.execute_reply": "2023-08-29T14:13:52.612248Z",
     "shell.execute_reply.started": "2023-08-29T14:13:50.645674Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "n_train = 3000\n",
    "n_test = 1000\n",
    "X_train = X_train[:n_train]\n",
    "y_train = y_train[:n_train]\n",
    "X_test = X_test[:n_test]\n",
    "y_test = y_test[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:19:47.992608Z",
     "iopub.status.busy": "2023-08-29T14:19:47.992296Z",
     "iopub.status.idle": "2023-08-29T14:23:08.198834Z",
     "shell.execute_reply": "2023-08-29T14:23:08.198112Z",
     "shell.execute_reply.started": "2023-08-29T14:19:47.992550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 300)          1500000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                85248     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,589,473\n",
      "Trainable params: 1,589,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "2400/2400 [==============================] - 51s 21ms/step - loss: 0.5956 - accuracy: 0.6550 - val_loss: 0.4370 - val_accuracy: 0.7900\n",
      "Epoch 2/100\n",
      "2400/2400 [==============================] - 48s 20ms/step - loss: 0.2551 - accuracy: 0.8963 - val_loss: 0.4643 - val_accuracy: 0.8283\n",
      "Epoch 3/100\n",
      "2400/2400 [==============================] - 49s 20ms/step - loss: 0.1045 - accuracy: 0.9663 - val_loss: 0.5729 - val_accuracy: 0.8433\n",
      "Epoch 4/100\n",
      "2400/2400 [==============================] - 48s 20ms/step - loss: 0.0512 - accuracy: 0.9842 - val_loss: 0.6679 - val_accuracy: 0.7983\n",
      "1000/1000 [==============================] - 3s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7020515403747558, 0.7929999828338623]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sieci z uczonymi embeddingami\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_features,output_dim=n_embeddings,input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(64, activation=\"tanh\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "model.fit(X_train, y_train, epochs=100, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-08-29T14:17:57.196123Z",
     "iopub.status.busy": "2023-08-29T14:17:57.195795Z",
     "iopub.status.idle": "2023-08-29T14:17:57.209247Z",
     "shell.execute_reply": "2023-08-29T14:17:57.208207Z",
     "shell.execute_reply.started": "2023-08-29T14:17:57.196067Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,   14,   22,   16,   43,  530,  973, 1622, 1385,   65,  458,\n",
       "        4468,   66, 3941,    4,  173,   36,  256,    5,   25,  100,   43,\n",
       "         838,  112,   50,  670,    2,    9,   35,  480,  284,    5,  150,\n",
       "           4,  172,  112,  167,    2,  336,  385,   39,    4,  172, 4536,\n",
       "        1111,   17,  546,   38,   13,  447,    4,  192,   50,   16,    6,\n",
       "         147, 2025,   19,   14,   22,    4, 1920, 4613,  469,    4,   22,\n",
       "          71,   87,   12,   16,   43,  530,   38,   76,   15,   13, 1247,\n",
       "           4,   22,   17,  515,   17,   12,   16,  626,   18,    2,    5,\n",
       "          62,  386,   12,    8,  316,    8,  106,    5,    4, 2223,    2,\n",
       "          16,  480,   66, 3785,   33,    4,  130,   12,   16,   38,  619,\n",
       "           5,   25,  124,   51,   36,  135,   48,   25, 1415,   33,    6,\n",
       "          22,   12,  215,   28,   77,   52,    5,   14,  407,   16,   82,\n",
       "           2,    8,    4,  107,  117,    2,   15,  256,    4,    2,    7,\n",
       "        3766,    5,  723,   36,   71,   43,  530,  476,   26,  400,  317,\n",
       "          46,    7,    4,    2, 1029,   13,  104,   88,    4,  381,   15,\n",
       "         297,   98,   32, 2071,   56,   26,  141,    6,  194,    2,   18,\n",
       "           4,  226,   22,   21,  134,  476,   26,  480,    5,  144,   30,\n",
       "           2,   18,   51,   36,   28,  224,   92,   25,  104,    4,  226,\n",
       "          65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "         113,  103,   32,   15,   16,    2,   19,  178,   32], dtype=int32),\n",
       " ['[START]',\n",
       "  'this',\n",
       "  'film',\n",
       "  'was',\n",
       "  'just',\n",
       "  'brilliant',\n",
       "  'casting',\n",
       "  'location',\n",
       "  'scenery',\n",
       "  'story',\n",
       "  'direction',\n",
       "  \"everyone's\",\n",
       "  'really',\n",
       "  'suited',\n",
       "  'the',\n",
       "  'part',\n",
       "  'they',\n",
       "  'played',\n",
       "  'and',\n",
       "  'you',\n",
       "  'could',\n",
       "  'just',\n",
       "  'imagine',\n",
       "  'being',\n",
       "  'there',\n",
       "  'robert',\n",
       "  '[OOV]',\n",
       "  'is',\n",
       "  'an',\n",
       "  'amazing',\n",
       "  'actor',\n",
       "  'and',\n",
       "  'now',\n",
       "  'the',\n",
       "  'same',\n",
       "  'being',\n",
       "  'director',\n",
       "  '[OOV]',\n",
       "  'father',\n",
       "  'came',\n",
       "  'from',\n",
       "  'the',\n",
       "  'same',\n",
       "  'scottish',\n",
       "  'island',\n",
       "  'as',\n",
       "  'myself',\n",
       "  'so',\n",
       "  'i',\n",
       "  'loved',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'there',\n",
       "  'was',\n",
       "  'a',\n",
       "  'real',\n",
       "  'connection',\n",
       "  'with',\n",
       "  'this',\n",
       "  'film',\n",
       "  'the',\n",
       "  'witty',\n",
       "  'remarks',\n",
       "  'throughout',\n",
       "  'the',\n",
       "  'film',\n",
       "  'were',\n",
       "  'great',\n",
       "  'it',\n",
       "  'was',\n",
       "  'just',\n",
       "  'brilliant',\n",
       "  'so',\n",
       "  'much',\n",
       "  'that',\n",
       "  'i',\n",
       "  'bought',\n",
       "  'the',\n",
       "  'film',\n",
       "  'as',\n",
       "  'soon',\n",
       "  'as',\n",
       "  'it',\n",
       "  'was',\n",
       "  'released',\n",
       "  'for',\n",
       "  '[OOV]',\n",
       "  'and',\n",
       "  'would',\n",
       "  'recommend',\n",
       "  'it',\n",
       "  'to',\n",
       "  'everyone',\n",
       "  'to',\n",
       "  'watch',\n",
       "  'and',\n",
       "  'the',\n",
       "  'fly',\n",
       "  '[OOV]',\n",
       "  'was',\n",
       "  'amazing',\n",
       "  'really',\n",
       "  'cried',\n",
       "  'at',\n",
       "  'the',\n",
       "  'end',\n",
       "  'it',\n",
       "  'was',\n",
       "  'so',\n",
       "  'sad',\n",
       "  'and',\n",
       "  'you',\n",
       "  'know',\n",
       "  'what',\n",
       "  'they',\n",
       "  'say',\n",
       "  'if',\n",
       "  'you',\n",
       "  'cry',\n",
       "  'at',\n",
       "  'a',\n",
       "  'film',\n",
       "  'it',\n",
       "  'must',\n",
       "  'have',\n",
       "  'been',\n",
       "  'good',\n",
       "  'and',\n",
       "  'this',\n",
       "  'definitely',\n",
       "  'was',\n",
       "  'also',\n",
       "  '[OOV]',\n",
       "  'to',\n",
       "  'the',\n",
       "  'two',\n",
       "  'little',\n",
       "  '[OOV]',\n",
       "  'that',\n",
       "  'played',\n",
       "  'the',\n",
       "  '[OOV]',\n",
       "  'of',\n",
       "  'norman',\n",
       "  'and',\n",
       "  'paul',\n",
       "  'they',\n",
       "  'were',\n",
       "  'just',\n",
       "  'brilliant',\n",
       "  'children',\n",
       "  'are',\n",
       "  'often',\n",
       "  'left',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  '[OOV]',\n",
       "  'list',\n",
       "  'i',\n",
       "  'think',\n",
       "  'because',\n",
       "  'the',\n",
       "  'stars',\n",
       "  'that',\n",
       "  'play',\n",
       "  'them',\n",
       "  'all',\n",
       "  'grown',\n",
       "  'up',\n",
       "  'are',\n",
       "  'such',\n",
       "  'a',\n",
       "  'big',\n",
       "  '[OOV]',\n",
       "  'for',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'film',\n",
       "  'but',\n",
       "  'these',\n",
       "  'children',\n",
       "  'are',\n",
       "  'amazing',\n",
       "  'and',\n",
       "  'should',\n",
       "  'be',\n",
       "  '[OOV]',\n",
       "  'for',\n",
       "  'what',\n",
       "  'they',\n",
       "  'have',\n",
       "  'done',\n",
       "  \"don't\",\n",
       "  'you',\n",
       "  'think',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'story',\n",
       "  'was',\n",
       "  'so',\n",
       "  'lovely',\n",
       "  'because',\n",
       "  'it',\n",
       "  'was',\n",
       "  'true',\n",
       "  'and',\n",
       "  'was',\n",
       "  \"someone's\",\n",
       "  'life',\n",
       "  'after',\n",
       "  'all',\n",
       "  'that',\n",
       "  'was',\n",
       "  '[OOV]',\n",
       "  'with',\n",
       "  'us',\n",
       "  'all'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sprawdźmy, czy inverted_word_index, którego chcemy wykorzystać przy embeddingach poprawnie mapuje liczbę na słowo \n",
    "X_train[0][182:], [inverted_word_index[x] for x in X_train[0][182:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:24:00.863162Z",
     "iopub.status.busy": "2023-08-29T14:24:00.862647Z",
     "iopub.status.idle": "2023-08-29T14:28:24.393631Z",
     "shell.execute_reply": "2023-08-29T14:28:24.392671Z",
     "shell.execute_reply.started": "2023-08-29T14:24:00.862961Z"
    }
   },
   "outputs": [],
   "source": [
    "# z wykorzystywanego notebooka - wczytywanie gotowych embeddingów:\n",
    "import numpy as np\n",
    "\n",
    "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:29:25.769383Z",
     "iopub.status.busy": "2023-08-29T14:29:25.768804Z",
     "iopub.status.idle": "2023-08-29T14:29:25.777996Z",
     "shell.execute_reply": "2023-08-29T14:29:25.776861Z",
     "shell.execute_reply.started": "2023-08-29T14:29:25.769158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 4999)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(inverted_word_index.keys()), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:30:15.646101Z",
     "iopub.status.busy": "2023-08-29T14:30:15.645771Z",
     "iopub.status.idle": "2023-08-29T14:30:15.652223Z",
     "shell.execute_reply": "2023-08-29T14:30:15.651439Z",
     "shell.execute_reply.started": "2023-08-29T14:30:15.646040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('resulting', 'spain', 'bergman')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_word_index[5000], inverted_word_index[5001], inverted_word_index[5002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:33:30.555927Z",
     "iopub.status.busy": "2023-08-29T14:33:30.555589Z",
     "iopub.status.idle": "2023-08-29T14:33:30.562247Z",
     "shell.execute_reply": "2023-08-29T14:33:30.561418Z",
     "shell.execute_reply.started": "2023-08-29T14:33:30.555854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bergman'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 słowa: ('resulting', 'spain', 'bergman') są nieużywane w X_train, bo X_train zawiera jedynie słowa o id równym 4999 i żadnych o większym id\n",
    "# zatem można je wyrzucić z naszego inverted_word_index\n",
    "inverted_word_index.pop(5000)\n",
    "inverted_word_index.pop(5001)\n",
    "inverted_word_index.pop(5002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:36:15.776094Z",
     "iopub.status.busy": "2023-08-29T14:36:15.775770Z",
     "iopub.status.idle": "2023-08-29T14:36:15.781928Z",
     "shell.execute_reply": "2023-08-29T14:36:15.781072Z",
     "shell.execute_reply.started": "2023-08-29T14:36:15.776038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(inverted_word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:36:16.849021Z",
     "iopub.status.busy": "2023-08-29T14:36:16.848710Z",
     "iopub.status.idle": "2023-08-29T14:36:16.870647Z",
     "shell.execute_reply": "2023-08-29T14:36:16.869932Z",
     "shell.execute_reply.started": "2023-08-29T14:36:16.848962Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index\n",
    "nb_words = max(inverted_word_index.keys())\n",
    "embedding_matrix = np.zeros((nb_words+1, n_embeddings))\n",
    "for i, word in inverted_word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-29T14:36:34.000686Z",
     "iopub.status.busy": "2023-08-29T14:36:34.000245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 300)          1500000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                85248     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,589,473\n",
      "Trainable params: 89,473\n",
      "Non-trainable params: 1,500,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/100\n",
      "2400/2400 [==============================] - 46s 19ms/step - loss: 0.6376 - accuracy: 0.6458 - val_loss: 0.5850 - val_accuracy: 0.7117\n",
      "Epoch 2/100\n",
      "2400/2400 [==============================] - 45s 19ms/step - loss: 0.4885 - accuracy: 0.7758 - val_loss: 0.4359 - val_accuracy: 0.8117\n",
      "Epoch 3/100\n",
      "2400/2400 [==============================] - 44s 19ms/step - loss: 0.6656 - accuracy: 0.6417 - val_loss: 0.6252 - val_accuracy: 0.6733\n",
      "Epoch 4/100\n",
      "2400/2400 [==============================] - 44s 18ms/step - loss: 0.5494 - accuracy: 0.7292 - val_loss: 0.4487 - val_accuracy: 0.7900\n",
      "Epoch 5/100\n",
      "2400/2400 [==============================] - 45s 19ms/step - loss: 0.4371 - accuracy: 0.8021 - val_loss: 0.4098 - val_accuracy: 0.8250\n",
      "Epoch 6/100\n",
      "2400/2400 [==============================] - 44s 18ms/step - loss: 0.3977 - accuracy: 0.8350 - val_loss: 0.4023 - val_accuracy: 0.8233\n",
      "Epoch 7/100\n",
      "2400/2400 [==============================] - 45s 19ms/step - loss: 0.3576 - accuracy: 0.8475 - val_loss: 0.3946 - val_accuracy: 0.8350\n",
      "Epoch 8/100\n",
      "2400/2400 [==============================] - 44s 18ms/step - loss: 0.3275 - accuracy: 0.8617 - val_loss: 0.4324 - val_accuracy: 0.8000\n",
      "Epoch 9/100\n",
      "2400/2400 [==============================] - 44s 18ms/step - loss: 0.3343 - accuracy: 0.8442 - val_loss: 0.3746 - val_accuracy: 0.8483\n",
      "Epoch 10/100\n",
      "2400/2400 [==============================] - 45s 19ms/step - loss: 0.2778 - accuracy: 0.8854 - val_loss: 0.3778 - val_accuracy: 0.8450\n",
      "Epoch 11/100\n",
      "2400/2400 [==============================] - 44s 18ms/step - loss: 0.2892 - accuracy: 0.8783 - val_loss: 0.3969 - val_accuracy: 0.8433\n",
      "Epoch 12/100\n",
      "2400/2400 [==============================] - 45s 19ms/step - loss: 0.2617 - accuracy: 0.8867 - val_loss: 0.3906 - val_accuracy: 0.8383\n",
      "1000/1000 [==============================] - 3s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36977984714508055, 0.8560000061988831]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# trainable=False - dzięki temu kod będzie trenował się szybciej, jest to jeden z głównych powodów używania gotowych embeddingów\n",
    "model.add(Embedding(input_dim=n_features,output_dim=n_embeddings,input_length=maxlen,\n",
    "                    weights = [embedding_matrix], trainable = False)) #using pre-trained embeddings\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(64, activation=\"tanh\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "model.fit(X_train, y_train, epochs=100, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
